<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[groupcache 设计原理剖析]]></title>
    <url>%2F2018%2F10%2F29%2Fgroupcache-%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[groupcache是一个用go实现的分布式k/v缓存及缓存填充库，它的作者也是memcached的作者，它已在Google多个生产环境中使用。它非常小巧精致，比较适用于分布式缓存的学习。它本身只是一个代码包（大约2000行代码，不需要配置服务器，在不同的请求处理场合，它可以充当客户端或者服务器的角色。它支持一致性哈希，即通过一致性哈希来对查询请求进行路由。对于缓存的具体策略，groupcache采用的是LRU，使用了一个List和一个Map来实现，非常简单。下面先简述本地缓存的基本模型和常见问题，然后剖析groupcache的设计原理。 单机缓存或者本地缓存是简单的，通过在内存中维护一个cache，当收到查询时，先查询cache是否已缓存查询结果，如果命中则直接返回，否则必须到存储系统执行查询，然后将结果先缓存到cache，然后返回结果。当然，这是本地缓存的基本模型，一般而言，缓存系统都面临着诸如缓存穿透、缓存雪崩及缓存击穿等问题。 缓存穿透指的是查询一定不存在的数据，此时从数据源查询不到结果，因此也无法对结果进行缓存，这直接导致此类型的查询请求每次都会落到数据源层，不仅使得缓存失效，当请求数量过多时也会浪费资源。 缓存雪崩指的是大量的缓存的过期时间被设置为相同或近似，使得缓存失效时，所有的查询请求全部落地到数据源层，同样，此时数据源层存在服务不可用的可能性。 缓存击穿则指的是对于那些热点数据，在缓存失效时，高并发的查询请求也会导致后端数据源层崩溃。 对于groupcache的设计，文章从一致性哈希、缓存命名空间、热数据扩散以及缓存过滤几个方面进行阐述。 一致性哈希一致性哈希最初是在论文《Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web》中提出，目标是致力于解决因特网中的热点(Hot spot)问题，并真正应用于p2p环境，它弥补简单的哈希算法的不足。一般可以从四个方面来衡量哈希算法的适用性。 平衡性。平衡性即哈希的结果能够尽可能的分散到所有节点或缓冲，以保证缓冲空间被最大程度使用。 单调性。单调性即如果当前缓存系统已经存在被映射的缓冲内容，当有新的节点加入到系统时，哈希算法应该能够尽可能保证原有已分配的的缓冲内容只能被映射到原有的对应节点或者新的节点，而不能被映射到旧的节点集合的其它节点。 分散性。分布式环境中，不同终端所见的节点范围有可能不同（因为可能只能看见部分节点），这会导致不同终端的哈希结果不一致，最终，相同的内容被映射到了不同的节点。而分散性则专门用于描述此种情况发生的严重程度。好的哈希算法应该尽量避免发生这种情况，即降低分散性。 负载。本质上与分散性阐述的是同一问题。但它从节点出发，即某一特定的节点应该尽可能被相同的缓冲内容所映射到，换言之，避免（不同终端）将相同的内容映射到不同的节点。 所谓一致性哈希，简而言之，即将节点与缓冲内容分别映射到一个巨大的环形空间中，最终内容的缓存节点为在顺时针方向上最靠近它的节点。可以发现，系统中节点的添加与删除，一致性哈希算法仍能基本满足以上四个特性。另外一个关键问题是，当集群中节点数量较少时，节点分布不均匀（即节点所负责的内容范围相差较大）会直接导致内容（数据）倾斜，因此一般会引入虚拟节点，即将节点映射为虚拟节点。如此，整个缓存映射过程便拆分为两个阶段：对于特定缓冲内容，先找到其映射的虚拟节点，然后再由虚拟节点映射到物理节点。 一致性哈希在分布式缓存中充当查询路由角色，因为不同节点负责特定的key集合。因此，如果此时当查询没能在本节点缓存中命中时，则需通过一致性哈希路由特定节点(peer)，然后借助http发送数据查询请求，请求的协议格式为: GET http://peer/key。因此，所有节点必须监听其它节点的数据查询请求，同时具备相应的请求处理模块。 缓存命名空间即便是在单个节点上，也可以创建若干个不同名称的缓存命名空间，以使得不同命名空间的缓存相互独立。如此，可以在原本针对key进行分片的基础上，丰富缓存功能。因此，节点间的数据查询请求协议格式变更为：GET http://peer/groupname/key。 热点数据扩散分布式缓存系统，不同的节点会负责特定的key集合的查询请求。但因为并非所有的key的访问量是均匀的，因此，存在这种情况：某些key属于热点数据而被大量访问，这可能导致包含该key的节点无法及时处理甚至瘫痪。考虑到这一点，groupcache增加了热点数据自动扩展的功能。即针对每一个节点，除了会缓存本节点存在且大量被访问的key之外（缓存这些key的对象被称之为maincache），也会缓存那些不属于本节点，但同样被大量访问（发生大量地miss cache）的key，而缓存这些key的对象被称这为hotcache，如此便能缓解热点数据的查询请求集中某一个节点的问题。 缓存过滤机制groupcache的singleflight模块实现了缓存过滤机制。即在大量相同的请求并发访问时，若缓存未能命中，则会触发大量的Load过程。即所有的查询请求全部会落到数据源（如DB）或从其它节点加载数据，因此考虑到节点可靠性，此时DB存在因压力过大而导致服务不可用的情况，同时也浪费资源。groupcache设计所提供的解决方案是：尽管存在并发的查询，但能保证只有一个请求能够真正的转发到DB执行查询，而其余的请求都会阻塞等待，直至第一个请求的查询结果返回，同时，其它请求会使用第一个请求的查询结果，最后再返回给客户端。singleflight通过go的sync.WaitGroup实现同一时间相同查询请求的合并。 最后，虽然官方声称groupcache在很多场景下已经成为memcached的替代版，但其本身存在固有的”局限性”。 groupcache采用的是LRU缓存机制，使用List和Map实现，不支持过期机制（不支持设置过期时间），也没有明确的回收机制（只是简单地将队尾的数据移除），但能够控制缓存总大小在用户设置的阈值之下。 groupcache不支持set、update以及delete，即对于客户端而言，只能执行get操作。 groupcache针对key不支持多个版本的值。 总而言之，groupcache是一个值得学习的开源分布式缓存系统，通过阅读源码，一方面可以了解分布式缓存相关的设计原则，也能学习编程相关的设计经验。]]></content>
      <categories>
        <category>分布式系统</category>
        <category>分布式缓存</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>分布式缓存</tag>
        <tag>LRU缓存</tag>
        <tag>一致性哈希</tag>
        <tag>缓存过滤机制</tag>
        <tag>缓存击穿</tag>
        <tag>热点数据扩散</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Live Sequence Protocol 实现]]></title>
    <url>%2F2018%2F10%2F25%2FLive-Sequence-Protocol-%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[分布式环境中，网络不稳定导致消息（数据包）的传输存在乱序、重复和丢失的情况，同时，节点宕机也不可避免。如何优雅地处理这些问题，是构建一个健壮的分布式系统的关键。网络的复杂性使得数据包传输协议至关重要。低级别的IP协议提供不可靠的数据报服务，即消息可能延时、重复或丢失，另外，它也限制了在网络节点中传输的消息的最大字节数，因此很少直接利用IP协议来构建分布式应用。在传输层，UDP也不提供可靠的数据报服务，但它可以通过端口定向传输报文。而TCP则会保证消息传输的可靠性、有序性，并允许任意字节大小的消息传递，还提供额外的功能，如流量控制、拥塞控制。 我们的目的是实现一个基于UDP、具备TCP几个关键特性的消息传输协议 (Live Sequence Protocol），同时它还具备如下功能： 不同于UDP或TCP，它支持 client-server通信模型 。 server会维护若干个client的连接。 server与client的通信是通过向对方发送消息来实现，消息大小限制与UDP相同。 消息传输是可靠的：消息一旦发送，就会被顺序接收，且每个消息只会被接收一次。 server与client可以检测连接的状态。 协议具体的工作原理、关键特性、运行流程及开放使用的接口可以参考p1.pdf。下面我会讨论协议实现过程中的几个关键点，以及个人在实现过程中遇到的棘手的问题。 系统逻辑框架构建清晰且优雅地构建整个系统的逻辑框架至关重要，代码框架设计关系到后期功能模块调试与扩展，不合理的系统逻辑框架设计会使得后期的扩展寸步难行，也会导致代码的可调试性、可读性变差。因此，在编写出你的第一个可用的版本之前，尽可能合理地安排系统框架，这需要理解并梳理系统的主干及各分支（异常）运行流程，为了更简单、高效且合理地实现模块功能，必须尽可能熟悉(go)语言的特性(channel、gorountine及interface)。 协议实现文档清晰地描述了协议的完整工作流程，按照此流程，其核心是epoch event触发后，协议的应对逻辑，可以实现出一个可运行的版本。合理安排程序框架关键在于处理好以下三个方面的问题： 哪些功能逻辑应该被顺序执行，如何保证同步顺序执行。比如，当创建client后，只有当其与server建立连接connection（抽象连接，并非消息传输所使用的连接）后才能返回，同时启动后台服务。注意client创建UDP连接到server可能会尝试多次，因为server可能存在慢启动问题，而且Connect消息也可能丢失。 系统需要哪些后台服务(background goroutine)， 后台服务如何可靠地同主线程协调交互。比如，对于client而言，至少需要三个goroutine来处理消息。 read goroutine持续从连接中读取消息，直到连接关闭。 write goroutine，因为写操作的调用是非阻塞的，但由于滑动窗口大小限制，并非所有消息都能立刻cache到滑动窗口并立即发送出去，因此，可以将用户wirte的消息放入到消息的write channel中，然后由专门的后台服务从channel中取消息，并在恰当的时候发送消息。 epoch event trigger goroutine，即处理与epoch相关的逻辑，超时如何处理？接收到Ack消息或Data消息如何处理？达到max epoch时如何处理？ 确保开放接口的实现符合协议规范中预定义的准则要求。比如，server的Read接口的调用会阻塞直到其从任一client收到消息，然后返回消息的payload及对应的connection ID。如果连接丢失，关闭或者Server主动关闭终止，都应该返回错误提示。这个方法不应该被简单地设计成从连接中持续读取数据，因为Server可能连接多个client，针对每一个client 连接的读取，必须启用单独的goroutine。所以，一种简单的设计是server并发地从各连接读取数据，若通过了校验（如保证用户调用Read所返回的数据正是用户所期望的），则将数据放入到channel，让Read持续从channel中取数据，注意数据一旦添加到channel中，则会以放入的顺序被Read取出，并返回给用户。 理解UDP通信本质大家可能对TCP原理及编程更为熟悉，UDP相对简单，但因为lsp(Live Sequence Protocol)基于UDP，并在更高的协议抽象层面具备TCP的特性，所以，不要混淆了二者的通信原理。UDP是无连接的！它会完全按照程序员的意愿发送消息，它不考虑对方主机是否存在或正常工作，也不会主动重发消息，因此，也就无法保证消息的可靠接收与发送。 所以，server不需要也不能维护其与client的连接！但应当在sever端创建并维护与其通信的client关联的信息实体（需包含哪些数据？），那何时创建？答案是当server读取到数据时，因为此时可以获取读取所返回的client地址，server可以通过cache已经连接的信息来判断此次读取对应的连接是否是新的连接。若不是，则直接进入消息读取处理逻辑，否则需要先初始化server维护的client相关联的信息实体。 最后，注意server与client使用的是不同的UDP读写通信接口。（client直接持有与server通信的连接，而server是通过指定地址（IP+port）发送与接收消息）。 如何实现滑动窗口滑动窗口sliding window是协议实现流量控制的关键，是整个协议的功能核心，并且其与TCP的滑动窗口机制类似。关于滑动窗口，在理解它的工作原理后，重点考虑以下三个方面： 设计滑动窗口的数据结构。 消息应该被有序添加到滑动窗口。 发送消息窗口需要标识每一条消息是否已经被ack。 发送消息所关联的滑动窗口latestSentDataMsg。以client作为示例，维持其发送消息的窗口，以便对未按时返回Ack的消息进行重发（已发送的data消息可能会丢失，或者接收主机响应的Ack消息丢失）。 因为窗口内的消息所返回的Ack是无序的（消息异步发送，网络传输也不能保证消息按序到达），所以，需要维护一个指针，表示当前返回的Ack消息的最小的序号receivedAckSeqNum，以作为窗口向前推进的依据。 当client发送data消息时，需同时将其cache到latestSentDataMsg。而当其接收到Ack消息时，需要执行更新此指针receivedAckSeqNum的逻辑。而server则需要对其所维护的每一个连接构建对应的发送消息窗口，但处理逻辑类似。 接收消息所关联的滑动窗口latestReceivedDataMsg。同样以client作为示例，维持其接收消息的窗口，以便在计时器超时后，对最近收到的若干个data消息，重发Ack消息。 同样，接收消息窗口也是无序的，因此，为了保证返回给用户的消息有序，需要维护一个指针，表示下一个期望接收到的data消息序号nextReceiveDataSeqNum（或者是当前已经接收到的最大的data消息的消息序号），它是依次递增的。对于接收到的任何data消息，若其SeqNum在此指针之后，都应该直接添加（暂时缓存）到latestReceivedDataMsg中，而不应该作为Read调用的返回结果。 当client收到server的data消息时，也需要将其cache到latestReceivedDataMsg，并判断是否需要更新nextReceiveDataSeqNum，若需要更新，则应当将更新过程中所涉及到的cache在接收消息窗口中的data消息按序添加到供Read接口所读取的channel。server同样是一个连接对应一个接收消息窗口。 如何实现流量控制流量控制表示若当前主机有过多的消息未被ack（网络拥塞），因此发送主机需要对用户调用Write接口的data消息进行阻塞以延缓发送。其实现关键是滑动窗口机制。具体实现原理为： 当用户调用Write接口以发送消息时，将消息添加到消息发送队列channel，然后返回，不能阻塞。 后台服务write goroutine从消息发送队列中不断的取消息，但在消息正式发送前，需要检测消息发送滑动窗口是否空闲idle，并且包含多少空闲的slot。 空闲的slot数目可以根据以下表达式计算：idleSlotNum := cli.params.WindowSize-(lastMsg.SeqNum-cli.receivedAckSeqNum)，其中，lastMsg为消息发送窗口中最后一个消息，即SeqNum最大的消息。 如果idleSlotNum大于0，则可以发送对应数目的消息，并将已经发送的消息记录到消息发送窗口，同时递增nextSendDataSeqNum指针。否则，write goroutine应该被阻塞住。那如何解除阻塞？每当client在接收到Ack消息时都要去尝试解除阻塞。 如图所示，当滑动窗口处于(a)的情况下，当用户调用Write以发送消息时，消息会被阻塞在write channel中，因为此时receivedAckSeqNum为9，消息发送窗口的idle的slot数目为：5-(14-9)=0。而当client接收若干Ack消息后，滑动窗口转移到(b)状态时，注意到receivedAckSeqNum从9逐一递增到11，消息发送窗口idleSlotNum为：5-(14-11)=2，因此窗口前移，并可以从write channel中顺序取出两个消息，进行发送。 如何检测消息重复消息重复主要包括data和Ack消息的重复接收。以client作为示例。 data消息的重复接收。 当client读取到data消息后，需要判断消息是否已经接收过。若消息重复，则直接返回Ack消息，否则应该先将消息cache到latestReceivedDataMsg。 可以通过消息的SeqNum来去重。这涉及两种情况：其一，消息已经被Ack，并且已经从latestReceivedDataMsg中移除，我们称之为消息被丢弃(discarded)。其二，消息被Ack，但仍然cache在latestReceivedDataMsg中。 Ack消息的重复接收。Ack消息的去重逻辑同data消息类似。 如何保证消息顺序发送主机异步发送消息，且消息在网络中传输也有不同程度的延迟，因此接收主机接收的消息序列的顺序很可能与发送主机发送的消息顺序不同。如何保证消息顺序？准确而言，如何以发送主机发送消息的顺序来返回给用户。 针对具备滑动窗口机制的消息传输，可以保证滑动窗口前所接收的消息，即已经被discarded的消息肯定是有序返回给用户的。而滑动窗口内的消息，因为无法规避从网络中读取乱序消息的问题，但在读取到消息后可以控制以何种顺序将消息返回给用户。简单而言，将收到的data消息先cache在latestReceivedDataMsg中，然后通过指针nextReceiveDataSeqNum来判断是否应该将窗口中cache的消息返回给用户。 如何优雅地关闭连接保证连接优雅地关闭是一个非常棘手的问题。其中，相比于client端的连接关闭，server的关闭又更为复杂。协议规范清晰地描述了client及server在关闭连接时需要注意的问题。其核心是： 当存在pending消息时，需要将其处理完成（即需保证接收到Ack消息）。 同时，一旦data消息被加入到write channel，它必须保证最后能够被发送出去。client的关闭相对简单，具体处理逻辑为：当用户调用Close接口时，需要判断是否存在pending消息，如何检测？两个条件： 保证消息发送窗口的最后一个消息的SeqNum恰好为其持有的receivedAckSeqNum的值。 保证write channel中没有任何未被处理的消息。因此如果此时存在pending消息，Close会被阻塞。那如何解除阻塞？每当client在接收到Ack消息时都要去尝试解除阻塞。此外，值得注意的是，在阻塞的过程中，如果触发了max epoch event，则client应该立刻返回，因为这表明连接已经discarded，此时要么所有pending消息已被处理，要么server主动关闭了连接。server的CloseConn接口可以看作是client的Close接口的非阻塞版本。而Close接口需要协调所有的connection的关闭。同样，server的某个连接也可能到达max epoch，此时其对应的连接应该被关闭。当所有连接都关闭时，Close才能返回。在连接关闭时，需要及时退出对应的background goroutine。 需注意的细节问题往往一些编程方面的细节，包括逻辑漏洞或者被忽视的语法问题会造成很长时间的调试。而且，当通信过程中，数据交换复杂变得越发复杂时，很难从庞大的日志文件中找出错误的根源。个人在实现的过程中，遇到两个问题： dead lock。死锁很容易产生，一般有两个原因，其一，资源的相互持有，造成两个线程都无法向前推进。其二，没有正确嵌套使用锁，你需要清楚锁是否可重入。 buffered channel。其导致的问题比较隐蔽，你首先要明确是使用带缓冲的channel或者不带缓冲的channel，如果是buffered channel，你需要确定它的大小，如果你不确定缓冲区数量是否足够，建议设置的稍大一些，但这个前提是，必须在合适的时机清空buffered channel，避免在复用buffered channel之后导致逻辑受到影响。 最后，需要提醒的是，分布式程序异步、并发，且网络复杂的特性导致其很难debug。所以，尽可能设计完善的日志流程，以帮助跟踪未符合期望的执行逻辑，并定位问题。 另外，cmu提供较为完善的测试程序，如果程序出现问题，可以对某一个或几个子测试用例进行单独测试，熟悉测试用例代码，了解测试用例流程是有必要的。]]></content>
      <categories>
        <category>分布式系统</category>
        <category>传输协议</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>网络编程</tag>
        <tag>传输协议</tag>
        <tag>可靠服务</tag>
        <tag>流量控制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F10%2F24%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
